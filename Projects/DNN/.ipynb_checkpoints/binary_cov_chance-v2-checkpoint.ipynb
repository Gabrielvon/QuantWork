{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __context__ import *\n",
    "from __future__ import print_function, unicode_literals, division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取行情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "begdate = 20080601\n",
    "enddate = 20180601\n",
    "code = '000002.SZ'\n",
    "predict_period=5\n",
    "\n",
    "DB = gcrl.db('internal')\n",
    "DB.query_init({'code': code, 'begin': begdate, 'end': enddate, 'fields': '*'})\n",
    "DB.query.update({'dtype':'cycle', 'n_cycle': 1, 'freq': pycoraldb.D})\n",
    "rawdf = DB.getBar(4)\n",
    "# rawdf = db.getBar(code, 20180501, 20180601).toDataFrame()\n",
    "# gclean = gfc.clean_rdata(rawdf)\n",
    "# clean_df = gclean.remove_by_time('stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "i_quota = rawdf[['open', 'high', 'low', 'pre_close']]\n",
    "i_quota['close'] = rawdf['new_price']\n",
    "\n",
    "# begdate = 20050101\n",
    "# enddate = 20180505\n",
    "# code = '000002.SZ'\n",
    "# predict_period=5\n",
    "# qf = QuoteFeed(universe_ticker=[code],\n",
    "#                        begin_day = begdate,\n",
    "#                        end_day = enddate,\n",
    "#               tracking_freq = 86400,\n",
    "#               adjust_method = 'forward')\n",
    "# qf.load_feed()\n",
    "# raw_quota = qf.get_stock_quote()\n",
    "# qf = QuoteFeed(universe_ticker='000905.SH',  \n",
    "#                        begin_day = begdate,\n",
    "#                        end_day = enddate,\n",
    "#               tracking_freq = 86400,\n",
    "#               adjust_method = 'forward',\n",
    "#               is_index=True) #加一行这个\n",
    "            \n",
    "# qf.load_feed()\n",
    "# i_quota = qf.get_index_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_delta(data, predict_period=5):\n",
    "\n",
    "    data = data.dropna()\n",
    "    data.loc[:, 'd_high'] = (data.loc[:, 'high'] - data.loc[:, 'pre_close']) / data.loc[:, 'pre_close']\n",
    "    data.loc[:, 'd_low'] = (data.loc[:, 'low'] - data.loc[:, 'pre_close']) / data.loc[:, 'pre_close']\n",
    "    data.loc[:, 'd_open'] = (data.loc[:, 'open'] - data.loc[:, 'pre_close']) / data.loc[:, 'pre_close']\n",
    "    data.loc[:, 'd_close'] = (data.loc[:, 'close'] - data.loc[:, 'pre_close']) / data.loc[:, 'pre_close']\n",
    "    data.loc[:, 'expr03'] = data.loc[:, 'close'].shift(-predict_period) / data.loc[:, 'close'] - 1\n",
    "    data = data.dropna()\n",
    "\n",
    "    data_d = data.loc[:, ['d_high', 'd_low', 'd_open', 'd_close']]\n",
    "    data_d = (data_d * 1000).astype(int)\n",
    "    data_d += 100\n",
    "\n",
    "    data_d.loc[:, 'd_oc'] = data_d.loc[:, 'd_close'] - data_d.loc[:, 'd_open']\n",
    "    data_d.loc[:, 'expr03'] = data.loc[:, 'expr03']\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_delta1(df, lookforward=5):\n",
    "    op, hi, lo, cl, pre_cl = df[['open', 'high', 'low', 'new_price', 'pre_close']].values.T\n",
    "    d_arr = np.stack([op, hi, lo, cl], 1)\n",
    "    d_arr = ((d_arr/pre_cl.reshape(-1, 1) - 1) * 1000).astype(int) + 100\n",
    "    d_oc = (d_arr[:, 3] - d_arr[:, 0]).reshape(-1, 1)\n",
    "    expr = np.full(d_oc.shape, np.nan)\n",
    "    expr[:-lookforward, 0] = cl[lookforward:] / cl[:-lookforward] - 1\n",
    "    out_df = pd.DataFrame(np.hstack([d_arr, d_oc]), index=df.index, \n",
    "                          columns=['d_open', 'd_high', 'd_low', 'd_close', 'd_oc'])\n",
    "    out_df.loc[:, 'expr03'] = expr\n",
    "    return out_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_delta2(df, lookforward=5):\n",
    "    arr_ohlc = df[['open', 'high', 'low', 'new_price']].values\n",
    "    pre_cl = df[['pre_close']].values\n",
    "    d_arr = arr_ohlc / pre_cl - 1\n",
    "    d_arr = (d_arr * 1000).astype(int) + 100\n",
    "    d_oc = (d_arr[:, 3] - d_arr[:, 0]).reshape(-1, 1)\n",
    "    expr = np.full(d_oc.shape, np.nan)\n",
    "    expr[:-lookforward, 0] = arr_ohlc[lookforward:, -1] / arr_ohlc[:-lookforward, -1] - 1\n",
    "    out_df = pd.DataFrame(np.hstack([d_arr, d_oc]), index=df.index, \n",
    "                          columns=['d_open', 'd_high', 'd_low', 'd_close', 'd_oc'])\n",
    "    out_df.loc[:, 'expr03'] = expr\n",
    "    return out_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_delta3(df , lookforward=5):\n",
    "    d_df = df[['open', 'high', 'low', 'new_price']].divide(df['pre_close'], axis=0) - 1\n",
    "    d_df.columns = ['d_open', 'd_high', 'd_low', 'd_close']\n",
    "    d_df = (d_df * 1000).astype(int) + 100\n",
    "    d_df['d_oc'] = df['new_price'] - df['open']\n",
    "    d_df['expr03'] = df['new_price'].pct_change(periods=lookforward).shift(-lookforward)\n",
    "    return d_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "6.26 ms ± 677 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "%timeit get_data_delta(i_quota)\n",
    "%timeit get_data_delta1(rawdf)\n",
    "# %timeit get_data_delta2(rawdf)\n",
    "# %timeit get_data_delta3(rawdf)\n",
    "\n",
    "data_d = get_data_delta(i_quota.copy())\n",
    "data_d0 = get_data_delta1(rawdf)\n",
    "print(np.allclose(data_d, data_d0))\n",
    "# 函数中存在rounding，因此会有微弱区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile data\n",
    "def compile_data(data_d):\n",
    "    channel_1 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    channel_2 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    i = 0\n",
    "    for index, row in data_d.iterrows():\n",
    "        if row['d_open'] > row['d_close']:\n",
    "            min_point = int(row['d_close'])\n",
    "            max_point = int(row['d_open'])\n",
    "        else:\n",
    "            min_point = int(row['d_open'])\n",
    "            max_point = int(row['d_close'])\n",
    "\n",
    "        channel_1[i][min_point:max_point + 1] = 1\n",
    "        channel_2[i][int(row['d_low']):int(row['d_high'] + 1)] = 1\n",
    "        i += 1\n",
    "\n",
    "    features = np.stack((channel_1, channel_2), axis=-1)\n",
    "    #features=np.reshape(channel_1, [-1, 201,1])\n",
    "    # print('feature shape', features.shape)\n",
    "    # append y\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data1(data_d):\n",
    "    channel_1 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    channel_2 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_begin = np.arange(data_d.shape[0]) * 201\n",
    "\n",
    "    lb1 = data_d[['d_open', 'd_close']].min(1).values\n",
    "    ub1 = data_d[['d_open', 'd_close']].max(1).values + 1\n",
    "    channel_1 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_to_fill_chan1 = [np.arange(begi, endi) for begi, endi in zip(idx_begin + lb1, idx_begin + ub1)]\n",
    "    np.put(channel_1, np.hstack(idx_to_fill_chan1), 1)\n",
    "\n",
    "    lb2 = data_d['d_low'].values\n",
    "    ub2 = data_d['d_high'].values + 1\n",
    "    channel_2 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_to_fill_chan2 = [np.arange(begi, endi) for begi, endi in zip(idx_begin + lb2, idx_begin + ub2)]\n",
    "    np.put(channel_2, np.hstack(idx_to_fill_chan2), 1)\n",
    "\n",
    "    features = np.stack((channel_1, channel_2), axis=-1)\n",
    "    # print('feature shape', features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 ms ± 13.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "34 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compile_data(data_d)\n",
    "%timeit compile_data1(data_d)\n",
    "features = compile_data(data_d)\n",
    "features1 = compile_data1(data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_batch_2d(features, ys, time_step):\n",
    "    data_x = []  # 训练集x和y初定义\n",
    "    data_y = []\n",
    "    for i in range(features.shape[0] - time_step + 1):\n",
    "        x = features[i:i + time_step]\n",
    "        y = ys[i + time_step - 1]\n",
    "        data_x.append(x.tolist())\n",
    "        data_y.append(y.tolist())\n",
    "\n",
    "    return np.array(data_x), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_batch_2d1(features, ys, time_step):\n",
    "    features_T = features.transpose((1, 2, 0))\n",
    "    features_T_rol = rolling_window(features_T, time_step)\n",
    "    data_x = features_T_rol.transpose(2, 3, 0, 1)\n",
    "    data_y = ys[time_step-1:]\n",
    "    return np.array(data_x), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = data_d.loc[:, 'expr03']\n",
    "xx, yy = get_data_batch_2d(features, ys, 20)\n",
    "xx0, yy0 = get_data_batch_2d1(features, ys, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2415, 20, 201, 2) (2415, 20, 201, 2)\n",
      "(2415,) (2415,)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(xx.shape, xx.shape)\n",
    "print(yy.shape, yy0.shape)\n",
    "print(np.allclose(xx, xx0))\n",
    "print(np.allclose(yy, yy0, equal_nan =True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainning_data(quota, batch_size=100, period_step=20, predict_period=5):\n",
    "    data2 = quota.loc[:, ['high', 'low', 'open', 'close']]  # 'volume'\n",
    "    data_d2 = get_data_delta(data2, predict_period=predict_period)\n",
    "\n",
    "    predict_data2 = data_d2  # .loc[trainning_count:]\n",
    "    predict_features2 = compile_data(predict_data2)\n",
    "    _ys2 = predict_data2.loc[:, 'expr03'].values\n",
    "    predict_x2, predict_dump2 = get_data_batch_2d(predict_features2, _ys2, period_step)\n",
    "    rest = batch_size - (predict_x2.shape[0] % batch_size)\n",
    "    # print('predict_x:', predict_x2.shape)\n",
    "    total_predict_count2 = predict_x2.shape[0]\n",
    "    predict_x2 = np.concatenate([predict_x2, predict_x2[:rest]])\n",
    "    predict_dump2 = np.concatenate([predict_dump2, predict_dump2[:rest]])\n",
    "    return predict_x2, predict_dump2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainning_data1(quota, batch_size=100, period_step=20, predict_period=5):\n",
    "    data2 = quota.copy()\n",
    "    predict_data2 = get_data_delta1(data2, lookforward=predict_period)\n",
    "\n",
    "    predict_features2 = compile_data1(predict_data2)\n",
    "    _ys2 = predict_data2.loc[:, 'expr03'].values\n",
    "    predict_x2, predict_dump2 = get_data_batch_2d1(predict_features2, _ys2, period_step)\n",
    "    rest = batch_size - (predict_x2.shape[0] % batch_size)\n",
    "    # print('predict_x:', predict_x2.shape)\n",
    "\n",
    "    predict_x2 = np.concatenate([predict_x2, predict_x2[:rest]])\n",
    "    predict_dump2 = np.concatenate([predict_dump2, predict_dump2[:rest]])\n",
    "    return predict_x2, predict_dump2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.79 s ± 367 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "262 ms ± 16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_trainning_data(i_quota)\n",
    "%timeit get_trainning_data1(rawdf)\n",
    "data_x, data_y = get_trainning_data(i_quota)\n",
    "data_x0, data_y0 = get_trainning_data1(rawdf)\n",
    "print(np.allclose(data_x, data_x0))\n",
    "print(np.allclose(data_y, data_y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape (2428, 201, 2)\n",
      "predict_x: (2408, 20, 201, 2)\n",
      "训练集x (2000, 20, 201, 2)\n",
      "测试集x (500, 20, 201, 2)\n",
      "总共x 2500\n"
     ]
    }
   ],
   "source": [
    "# 将data_d乱序后拆分成trainning_set和test_set, 按80%分位拆分\n",
    "# data_t=np.random.shuffle(data_t)\n",
    "data_x, data_y = get_trainning_data(i_quota, 100, 20, 5)\n",
    "\n",
    "\n",
    "trainning_count = int(data_x.shape[0] * 0.8)\n",
    "test_count = data_x.shape[0] - trainning_count\n",
    "\n",
    "trainning_x = data_x[:trainning_count]\n",
    "trainning_y = data_y[:trainning_count]\n",
    "test_x = data_x[trainning_count:]\n",
    "test_y = data_y[trainning_count:]\n",
    "\n",
    "print('训练集x', trainning_x.shape)\n",
    "print('测试集x', test_x.shape)\n",
    "print('总共x', data_x.shape[0])\n",
    "assert(trainning_x.shape[0] + test_x.shape[0] == data_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/notebook/Workspace/colv_nn/temp/train_x2.npy', trainning_x, allow_pickle=False)\n",
    "np.save('/root/notebook/Workspace/colv_nn/temp/train_y2.npy', trainning_y, allow_pickle=False)\n",
    "np.save('/root/notebook/Workspace/colv_nn/temp/test_x2.npy', test_x, allow_pickle=False)\n",
    "np.save('/root/notebook/Workspace/colv_nn/temp/test_y2.npy', test_y, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/root/notebook/Workspace/colv_nn/temp/train_x2.npy')\n",
    "y_train = np.load('/root/notebook/Workspace/colv_nn/temp/train_y2.npy')\n",
    "X_test = np.load('/root/notebook/Workspace/colv_nn/temp/test_x2.npy')\n",
    "y_test = np.load('/root/notebook/Workspace/colv_nn/temp/test_y2.npy')\n",
    "\n",
    "\n",
    "#设置条件\n",
    "\n",
    "#y_test=(test_y>0.02).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train > 0.03, 1, np.where(y_train < -0.02, 2, 0)).astype(int)\n",
    "y_test = np.where(y_test > 0.03, 1, np.where(y_test < -0.02, 2, 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  binarynet/input: (100, 20, 201, 2)\n",
      "[TL] BinaryConv2d bcnn1: n_filter:32 filter_size:(5, 5) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool1: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn1: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] BinaryConv2d bcnn2: n_filter:64 filter_size:(5, 5) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool2: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn2: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] FlattenLayer binarynet/flatten: 16320\n",
      "[TL] BinaryDenseLayer  dense: 201 identity\n",
      "[TL] BatchNormLayer bn3: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] BinaryDenseLayer  bout: 3 identity\n",
      "[TL] BatchNormLayer bno: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "[TL] InputLayer  binarynet/input: (100, 20, 201, 2)\n",
      "[TL] BinaryConv2d bcnn1: n_filter:32 filter_size:(5, 5) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool1: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn1: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] BinaryConv2d bcnn2: n_filter:64 filter_size:(5, 5) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool2: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn2: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] FlattenLayer binarynet/flatten: 16320\n",
      "[TL] BinaryDenseLayer  dense: 201 identity\n",
      "[TL] BatchNormLayer bn3: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] BinaryDenseLayer  bout: 3 identity\n",
      "[TL] BatchNormLayer bno: decay:0.900000 epsilon:0.000010 act:identity is_train:False\n",
      "[TL]   [*] geting variables with binarynet\n",
      "[TL]   got   0: binarynet/bcnn1/W_conv2d:0   (5, 5, 2, 32)\n",
      "[TL]   got   1: binarynet/bn1/beta:0   (32,)\n",
      "[TL]   got   2: binarynet/bn1/gamma:0   (32,)\n",
      "[TL]   got   3: binarynet/bcnn2/W_conv2d:0   (5, 5, 32, 64)\n",
      "[TL]   got   4: binarynet/bn2/beta:0   (64,)\n",
      "[TL]   got   5: binarynet/bn2/gamma:0   (64,)\n",
      "[TL]   got   6: binarynet/dense/W:0   (16320, 201)\n",
      "[TL]   got   7: binarynet/bn3/beta:0   (201,)\n",
      "[TL]   got   8: binarynet/bn3/gamma:0   (201,)\n",
      "[TL]   got   9: binarynet/bout/W:0   (201, 3)\n",
      "[TL]   got  10: binarynet/bno/beta:0   (3,)\n",
      "[TL]   got  11: binarynet/bno/gamma:0   (3,)\n",
      "[TL]   param   0: binarynet/bcnn1/Sign:0 (5, 5, 2, 32)      float32 (mean: -0.007499999832361937, median: -1.0              , std: 0.9999719262123108)   \n",
      "[TL]   param   1: binarynet/bn1/beta:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: binarynet/bn1/gamma:0 (32,)              float32_ref (mean: 1.0001109838485718, median: 1.0001375675201416, std: 0.0018764312844723463)   \n",
      "[TL]   param   3: binarynet/bn1/moving_mean:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: binarynet/bn1/moving_variance:0 (32,)              float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param   5: binarynet/bcnn2/Sign:0 (5, 5, 32, 64)     float32 (mean: -0.0024609374813735485, median: -1.0              , std: 0.9999969005584717)   \n",
      "[TL]   param   6: binarynet/bn2/beta:0 (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   7: binarynet/bn2/gamma:0 (64,)              float32_ref (mean: 1.0003623962402344, median: 1.0001249313354492, std: 0.0019786800257861614)   \n",
      "[TL]   param   8: binarynet/bn2/moving_mean:0 (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   9: binarynet/bn2/moving_variance:0 (64,)              float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param  10: binarynet/dense/Sign:0 (16320, 201)       float32 (mean: 4.877572791883722e-06, median: 1.0               , std: 1.0               )   \n",
      "[TL]   param  11: binarynet/bn3/beta:0 (201,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  12: binarynet/bn3/gamma:0 (201,)             float32_ref (mean: 0.9998886585235596, median: 1.000015139579773 , std: 0.001993628451600671)   \n",
      "[TL]   param  13: binarynet/bn3/moving_mean:0 (201,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  14: binarynet/bn3/moving_variance:0 (201,)             float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param  15: binarynet/bout/Sign:0 (201, 3)           float32 (mean: 0.04145937040448189, median: 1.0               , std: 0.999140202999115 )   \n",
      "[TL]   param  16: binarynet/bno/beta:0 (3,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  17: binarynet/bno/gamma:0 (3,)               float32_ref (mean: 1.000740647315979 , median: 1.0008373260498047, std: 0.0007917949697002769)   \n",
      "[TL]   param  18: binarynet/bno/moving_mean:0 (3,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  19: binarynet/bno/moving_variance:0 (3,)               float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   num of params: 3334923\n",
      "[TL]   layer   0: binarynet/bcnn1/Identity:0 (100, 20, 201, 32)    float32\n",
      "[TL]   layer   1: binarynet/pool1/MaxPool:0 (100, 10, 101, 32)    float32\n",
      "[TL]   layer   2: binarynet/bn1/htanh:0 (100, 10, 101, 32)    float32\n",
      "[TL]   layer   3: binarynet/bcnn2/Identity:0 (100, 10, 101, 64)    float32\n",
      "[TL]   layer   4: binarynet/pool2/MaxPool:0 (100, 5, 51, 64)    float32\n",
      "[TL]   layer   5: binarynet/bn2/htanh:0 (100, 5, 51, 64)    float32\n",
      "[TL]   layer   6: binarynet/flatten:0  (100, 16320)       float32\n",
      "[TL]   layer   7: binarynet/dense/Identity:0 (100, 201)         float32\n",
      "[TL]   layer   8: binarynet/bn3/htanh:0 (100, 201)         float32\n",
      "[TL]   layer   9: binarynet/bout/Identity:0 (100, 3)           float32\n",
      "[TL]   layer  10: binarynet/bno/Identity_2:0 (100, 3)           float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 6 took 18.658706s\n",
      "   train loss: 1.108877\n",
      "   train acc: 0.430500\n",
      "Evaluation\n",
      "   test loss: 1.274167\n",
      "   test acc: 0.346000\n",
      "Epoch 2 of 6 took 18.504486s\n",
      "   train loss: 1.027687\n",
      "   train acc: 0.481000\n",
      "Evaluation\n",
      "   test loss: 1.206893\n",
      "   test acc: 0.320000\n",
      "Epoch 3 of 6 took 18.349102s\n",
      "   train loss: 0.934545\n",
      "   train acc: 0.567500\n",
      "Evaluation\n",
      "   test loss: 1.146362\n",
      "   test acc: 0.380000\n",
      "Epoch 4 of 6 took 18.599268s\n",
      "   train loss: 0.903319\n",
      "   train acc: 0.599000\n",
      "Evaluation\n",
      "   test loss: 1.184727\n",
      "   test acc: 0.376000\n",
      "Epoch 5 of 6 took 18.449298s\n",
      "   train loss: 0.846582\n",
      "   train acc: 0.631500\n",
      "Evaluation\n",
      "   test loss: 1.193822\n",
      "   test acc: 0.394000\n",
      "Epoch 6 of 6 took 18.572948s\n",
      "   train loss: 0.763142\n",
      "   train acc: 0.700000\n",
      "Evaluation\n",
      "   test loss: 1.201684\n",
      "   test acc: 0.410000\n",
      "Evaluation\n",
      "   test loss: 1.201684\n",
      "   test acc: 0.410000\n"
     ]
    }
   ],
   "source": [
    "if sess == None:\n",
    "    sess = tf.InteractiveSession()\n",
    "elif sess._closed == False:\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "def model(x, is_train=True, reuse=False):\n",
    "    # In BNN, all the layers inputs are binary, with the exception of the first layer.\n",
    "    # ref: https://github.com/itayhubara/BinaryNet.tf/blob/master/models/BNN_cifar10.py\n",
    "    with tf.variable_scope(\"binarynet\", reuse=reuse):\n",
    "        net = tl.layers.InputLayer(x, name='input')\n",
    "        net = tl.layers.BinaryConv2d(net, 32, (5, 5), (1, 1),\n",
    "                                     padding='SAME', b_init=None, name='bcnn1')\n",
    "        net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool1')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn1')\n",
    "\n",
    "        #net = tl.layers.SignLayer(net)\n",
    "        net = tl.layers.BinaryConv2d(net, 64, (5, 5), (1, 1),\n",
    "                                     padding='SAME', b_init=None, name='bcnn2')\n",
    "        net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool2')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn2')\n",
    "\n",
    "        net = tl.layers.FlattenLayer(net)\n",
    "        # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop1')\n",
    "        #net = tl.layers.SignLayer(net)\n",
    "        net = tl.layers.BinaryDenseLayer(net, 201, b_init=None, name='dense')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn3')\n",
    "\n",
    "        # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop2')\n",
    "        #net = tl.layers.SignLayer(net)\n",
    "        net = tl.layers.BinaryDenseLayer(net, 3, b_init=None, name='bout')\n",
    "        net = tl.layers.BatchNormLayer(net, is_train=is_train, name='bno')\n",
    "    return net\n",
    "\n",
    "\n",
    "# define inferences\n",
    "batch_size = 100\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 20, 201, 2])\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "net_train = model(x, is_train=True, reuse=False)\n",
    "net_test = model(x, is_train=False, reuse=True)\n",
    "\n",
    "\n",
    "# cost for training\n",
    "y = net_train.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "\n",
    "# cost and accuracy for evalution\n",
    "y2 = net_test.outputs\n",
    "prediction = tf.argmax(y2, 1)\n",
    "\n",
    "cost_test = tl.cost.cross_entropy(y2, y_, name='xentropy2')\n",
    "correct_prediction = tf.equal(tf.argmax(y2, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# define the optimizer\n",
    "train_params = tl.layers.get_variables_with_name('binarynet', True, True)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
    "\n",
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "net_train.print_params()\n",
    "net_train.print_layers()\n",
    "\n",
    "n_epoch = 6\n",
    "\n",
    "# print(sess.run(net_test.all_params)) # print real values of parameters\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
    "\n",
    "    if True:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "        print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "#         for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "#             err, ac = sess.run([cost_test, acc], feed_dict={x: X_val_a, y_: y_val_a})\n",
    "#             val_loss += err\n",
    "#             val_acc += ac\n",
    "#             n_batch += 1\n",
    "#         print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "#         print(\"   val acc: %f\" % (val_acc / n_batch))\n",
    "        print('Evaluation')\n",
    "        test_loss, test_acc, n_batch = 0, 0, 0\n",
    "        for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "            test_loss += err\n",
    "            test_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "        print(\"   test acc: %f\" % (test_acc / n_batch))\n",
    "\n",
    "\n",
    "print('Evaluation')\n",
    "test_loss, test_acc, n_batch = 0, 0, 0\n",
    "for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "    err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "    test_loss += err\n",
    "    test_acc += ac\n",
    "    n_batch += 1\n",
    "print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "print(\"   test acc: %f\" % (test_acc / n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape (2428, 201, 2)\n",
      "predict_x: (2408, 20, 201, 2)\n",
      "predict_x: (2500, 20, 201, 2)\n",
      "predict_y: (2500,)\n",
      "batch predict\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c2d492cdfca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#pred.append(sess.run(prediction, feed_dict={x: X_train_a}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_a\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal_predict_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测与实际图对比\n",
    "#fig, axs = plt.subplots(1,1,figsize = (25,8))\n",
    "data_d = get_data_delta(i_quota)\n",
    "predict_data = data_d  # .loc[trainning_count:]\n",
    "predict_features = compile_data(predict_data)\n",
    "_ys = predict_data.loc[:, 'expr03'].values\n",
    "\n",
    "#predict_y = sess.run([y2], feed_dict={p_x: predict_x})\n",
    "predict_features.shape\n",
    "predict_x, predict_dump = get_data_batch_2d(predict_features, _ys, 20)\n",
    "rest = batch_size - (predict_x.shape[0] % batch_size)\n",
    "print('predict_x:', predict_x.shape)\n",
    "total_predict_count = predict_x.shape[0]\n",
    "predict_x = np.concatenate([predict_x, predict_x[:rest]])\n",
    "predict_dump = np.concatenate([predict_dump, predict_dump[:rest]])\n",
    "after_fit = predict_x\n",
    "print('predict_x:', predict_x.shape)\n",
    "print('predict_y:', predict_dump.shape)\n",
    "for X_train_a, y_train_a in tl.iterate.minibatches(predict_x, predict_dump, batch_size, shuffle=False):\n",
    "    print('batch predict')\n",
    "    #pred.append(sess.run(prediction, feed_dict={x: X_train_a}))\n",
    "    pred.append(sess.run(y2, feed_dict={x: X_train_a}))\n",
    "\n",
    "pred = np.array(pred).reshape([-1, 3])[:total_predict_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_test:', X_test.shape)\n",
    "print('predict_x:', predict_x.shape)\n",
    "print('pred:', pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analy_pred_value(pred, i_quota, upchance=0.3):\n",
    "    pred_chance = pred[:, 1] > pred[:, 2]\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(25, 8))\n",
    "    ax1 = axs\n",
    "    pred_raw_data = i_quota.reset_index(drop=True)\n",
    "\n",
    "    pred_raw_data.loc[:, 'chance'] = pd.Series(pred_chance)\n",
    "    tmp_df = pred_raw_data.loc[:, ['close', 'chance']].reset_index(drop=True)\n",
    "    #tmp_df = tmp_df.reset_index(drop = True)\n",
    "    #tmp_df['predict_y'] = np.array(prediction)\n",
    "\n",
    "    # upchance = 0.3 #标记上涨概率大于0.2和下跌概率为-0.2的机会\n",
    "\n",
    "    ax1.plot(tmp_df.close)\n",
    "    ax1.plot(tmp_df.loc[tmp_df.chance > upchance, :].index,\n",
    "             tmp_df.loc[tmp_df.chance > upchance, :].close, 'r^')\n",
    "    ax1.plot(tmp_df.loc[tmp_df.chance < -upchance, :].index,\n",
    "             tmp_df.loc[tmp_df.chance < -upchance, :].close, 'bv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analy_pred_dir(pred, i_quota, upchance=0.3):\n",
    "    pred_chance_up = np.logical_and(pred[:, 1] > pred[:, 2], pred[:, 1] > pred[:, 0])\n",
    "    pred_chance_down = np.logical_and(pred[:, 2] > pred[:, 1], pred[:, 2] > pred[:, 0])\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(25, 8))\n",
    "    ax1 = axs\n",
    "    pred_raw_data = i_quota.reset_index(drop=True)\n",
    "\n",
    "    # pred_raw_data.loc[:,'chance']=pd.Series(pred_chance)\n",
    "    tmp_df = pred_raw_data  # pred_raw_data.loc[:,['close', 'chance']].reset_index(drop=True)\n",
    "    #tmp_df = tmp_df.reset_index(drop = True)\n",
    "    #tmp_df['predict_y'] = np.array(prediction)\n",
    "\n",
    "    # upchance = 0.3 #标记上涨概率大于0.2和下跌概率为-0.2的机会\n",
    "\n",
    "    ax1.plot(tmp_df.close)\n",
    "    ax1.plot(tmp_df.loc[pred_chance_up == True, :].index,\n",
    "             tmp_df.loc[pred_chance_up == True, :].close, 'r^')\n",
    "    ax1.plot(tmp_df.loc[pred_chance_down == True, :].index,\n",
    "             tmp_df.loc[pred_chance_down == True, :].close, 'bv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analy_pred_value(pred, i_quota, upchance = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analy_pred_dir(pred, i_quota, upchance = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用之前的模型预测中证500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begdate = 20180101\n",
    "enddate = 20180505\n",
    "code = '000300.SH'\n",
    "predict_period=5\n",
    "\n",
    "DB = gcrl.db('internal')\n",
    "DB.query_init({'code': code, 'begin': begdate, 'end': enddate, 'fields': '*'})\n",
    "DB.query.update({'dtype':'cycle', 'n_cycle': 1, 'freq': pycoraldb.D})\n",
    "rawdf = DB.getBar(4)\n",
    "# gclean = gfc.clean_rdata(rawdf)\n",
    "# clean_df = gclean.remove_by_time('stock')\n",
    "i_quota2 = rawdf.copy()\n",
    "i_quota2 = rawdf[['open', 'high', 'low', 'close', 'pre_close']]\n",
    "i_quota2['close'] = rawdf['new_price']\n",
    "\n",
    "# begdate = 20050101\n",
    "# enddate = 20180505\n",
    "# code = '000300.SH'\n",
    "# predict_period = 5\n",
    "# qf2 = QuoteFeed(universe_ticker=code,\n",
    "#                 begin_day=begdate,\n",
    "#                 end_day=enddate,\n",
    "#                 tracking_freq=86400,\n",
    "#                 is_index=True,\n",
    "#                 adjust_method='forward')\n",
    "# qf2.load_feed()\n",
    "# i_quota2 = qf2.get_index_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x2, predict_dump2 = get_trainning_data(i_quota2, 100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_features2.shape\n",
    "# predict_x2, predict_dump2=get_data_batch_2d(predict_features2,_ys2,20)\n",
    "# #predict_y = sess.run([y2], feed_dict={p_x: predict_x})\n",
    "# pred2=[]\n",
    "# rest = batch_size - (predict_x2.shape[0] % batch_size)\n",
    "# print('predict_x:', predict_x2.shape)\n",
    "#\n",
    "# predict_x2=np.concatenate([predict_x2, predict_x2[:rest]])\n",
    "# predict_dump2=np.concatenate([predict_dump2, predict_dump2[:rest]])\n",
    "# after_fit2=predict_x2\n",
    "# print('predict_x:', predict_x2.shape)\n",
    "# print('predict_y:', predict_dump2.shape)\n",
    "pred2 = []\n",
    "total_predict_count2 = predict_x2.shape[0]\n",
    "for X_train_a, y_train_a in tl.iterate.minibatches(predict_x2, predict_dump2, batch_size, shuffle=False):\n",
    "    pred2.append(sess.run(y2, feed_dict={x: X_train_a}))\n",
    "\n",
    "pred2 = np.array(pred2).reshape([-1, 2])[:total_predict_count2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analy_pred_value(pred2, i_quota2, upchance=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
