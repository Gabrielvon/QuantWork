{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Columns\n",
    ">['up_shadow1000','up_shadow1030','up_shadow1100','up_shadow1130','up_shadow1330','up_shadow1400','up_shadow1430','up_shadow1500']\n",
    "['down_shadow1000','down_shadow1030','down_shadow1100','down_shadow1130','down_shadow1330','down_shadow1400','down_shadow1430','down_shadow1500']\n",
    "['columnar_len1000','columnar_len1030','columnar_len1100','columnar_len1130','columnar_len1330','columnar_len1400','columnar_len1430','columnar_len1500']\n",
    "['ret1000', 'ret1030', 'ret1100', 'ret1130', 'ret1330', 'ret1400', 'ret1430', 'ret1500']\n",
    "\n",
    "- Calculation\n",
    "> y_df.loc[:,'up_shadow1000'] = (y_df['high1000'].astype(float)-max(y_df['open1000'].astype(float),y_df['close1000'].astype(float)))/y_df['close1000'].astype(float)\n",
    "y_df.loc[:,'down_shadow1000'] = (y_df['low1000'].astype(float)-min(y_df['open1000'].astype(float),y_df['close1000'].astype(float)))/y_df['close1000'].astype(float)\n",
    "y_df.loc[:,'columnar_len1000'] = (y_df['open1000'].astype(float)-y_df['close1000'].astype(float))/y_df['close1000'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.6.2 fengwl\n"
     ]
    }
   ],
   "source": [
    "from __context__ import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "from IPython.core.display import clear_output\n",
    "# pycoraldb.register_main_globals(globals())\n",
    "print(pycoraldb.__version__, db.username)\n",
    "client = db\n",
    "\n",
    "# from faeval.lft import portfolioTest, PerformanceAnalysis\n",
    "# from faeval import utility as ut\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_years(end, periods):\n",
    "    return pd.date_range(end=str(end), periods=periods, freq='Y').strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "def get_rawdata(begindate, enddate, columns=None):\n",
    "    dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "    if columns is not None:\n",
    "        field_list = ['date', 'code'] + columns\n",
    "        fields = ','.join(field_list)\n",
    "    else:\n",
    "        fields = ''\n",
    "\n",
    "    all_info = pd.DataFrame(db.getData(\"pengpy.testtable\", dt_filter, fields, password = '123456'))\n",
    "    all_info = all_info.dropna(how='all')\n",
    "\n",
    "    dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "    fields = 'date,code, adjustment_daily_ret'\n",
    "    return_info = pd.DataFrame(db.getData(\"gaoxin.new_factor_table\", dt_filter, fields, password='123456'))\n",
    "    return_info = return_info.dropna(how='all')\n",
    "\n",
    "    dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "    fields = ''\n",
    "    basic_info = pd.DataFrame(db.getData(\"gaoxin.Instruments_Daily_Basic_Info\", dt_filter, fields, password='123456'))\n",
    "    basic_info = basic_info.dropna(how='all')\n",
    "\n",
    "    rawdata = gfc.ijoin(all_info, [return_info, basic_info])\n",
    "    rawdata['date_int'] = rawdata['date'].copy()\n",
    "    rawdata['date'] = pd.to_datetime(rawdata['date'], format='%Y%m%d')\n",
    "    rawdata['daily_ret'] = rawdata['daily_ret'].astype(float)\n",
    "    rawdata[feature_colns+['adjustment_daily_ret']] = rawdata[feature_colns+['adjustment_daily_ret']].astype(float)\n",
    "    rawdata = gfc.compress_dataframe(rawdata)\n",
    "\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "years = get_years(2019, 10)\n",
    "feature_colns = ['up_shadow1000', 'up_shadow1030', 'up_shadow1100', 'up_shadow1130', 'up_shadow1330', 'up_shadow1400', 'up_shadow1430', 'up_shadow1500', 'down_shadow1000', 'down_shadow1030', 'down_shadow1100', 'down_shadow1130', 'down_shadow1330', 'down_shadow1400', 'down_shadow1430', 'down_shadow1500', 'columnar_len1000', 'columnar_len1030', 'columnar_len1100', 'columnar_len1130', 'columnar_len1330', 'columnar_len1400', 'columnar_len1430', 'columnar_len1500', 'ret1000', 'ret1030', 'ret1100', 'ret1130', 'ret1330', 'ret1400', 'ret1430', 'ret1500']\n",
    "data_path = '/root/notebook/Workspace/colv_nn/zz800net_data/'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "    \n",
    "for begindate, enddate in zip(years, years[1:]):\n",
    "    try:\n",
    "        fn = data_path + 'zz800_data_{}_{}.csv'.format(begindate[2:], enddate[2:])\n",
    "        if not os.path.exists(fn):\n",
    "            rawdata = get_rawdata(begindate, enddate, feature_colns)\n",
    "            rawdata.to_csv(fn)\n",
    "            print(begindate, enddate, ': ', gfc.mem_usage(rawdata), 'MB')\n",
    "        else:\n",
    "            print(begindate, enddate, ': existed', )\n",
    "    except Exception as e:\n",
    "        print(begindate, enddate, ': failed')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Params Initialized\n",
    "begindate, enddate = 20170601, 20180101\n",
    "# begindate, enddate = 20180501, 20180523\n",
    "rs = db.getTradingDays(begindate, enddate)\n",
    "datearr = pd.DataFrame(rs.values, columns=rs.columns)\n",
    "to_filter_code = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "feature_colns = ['up_shadow1000', 'up_shadow1030', 'up_shadow1100', 'up_shadow1130', 'up_shadow1330', 'up_shadow1400', 'up_shadow1430', 'up_shadow1500', 'down_shadow1000', 'down_shadow1030', 'down_shadow1100', 'down_shadow1130', 'down_shadow1330', 'down_shadow1400', 'down_shadow1430', 'down_shadow1500', 'columnar_len1000', 'columnar_len1030', 'columnar_len1100', 'columnar_len1130', 'columnar_len1330', 'columnar_len1400', 'columnar_len1430', 'columnar_len1500', 'ret1000', 'ret1030', 'ret1100', 'ret1130', 'ret1330', 'ret1400', 'ret1430', 'ret1500']\n",
    "field_list = ['date', 'code'] + feature_colns\n",
    "fields = ','.join(field_list)\n",
    "# fields = ''\n",
    "all_info = pd.DataFrame(db.getData(\"pengpy.testtable\", dt_filter, fields, password = '123456'))\n",
    "# all_info = all_info[feature_colns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upshadow = all_info.filter(like='up_shadow')\n",
    "# dnshadow = all_info.filter(like='down_shadow')\n",
    "# columnar = all_info.filter(like='columnar_len')\n",
    "# prichg = all_info.filter(regex='ret\\d+')\n",
    "# all_info = pd.concat([upshadow, dnshadow, columnar, prichg], 1)\n",
    "# print(np.hstack([upshadow.columns, dnshadow.columns, columnar.columns, prichg.columns]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "fields = 'date,code, adjustment_daily_ret'\n",
    "return_info = pd.DataFrame(db.getData(\"gaoxin.new_factor_table\", dt_filter, fields, password='123456'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_filter =  '_id = irange(\"{}\", \"{}\")'.format(begindate, enddate)\n",
    "fields = ''\n",
    "basic_info = pd.DataFrame(db.getData(\"gaoxin.Instruments_Daily_Basic_Info\", dt_filter, fields, password='123456'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __get_data(int_dates):\n",
    "#     for dt in int_dates:\n",
    "#         query = {'date': int(dt)}\n",
    "#         rs = db.getUserData('gaoxin.Instruments_Daily_Basic_Info', query, password = '123456')\n",
    "#         yield pd.DataFrame(rs)\n",
    "\n",
    "# old_basic_info = pd.concat(__get_data(datearr['date'].values))\n",
    "# old_basic_info = old_basic_info[['date', 'code', 'days_to_market']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data\n",
    "# rawdata = all_info.merge(return_info, on=['date', 'code'], how='outer')\n",
    "# rawdata = rawdata.merge(basic_info, on=['date', 'code'], how='outer')\n",
    "# rawdata = rawdata.merge(old_basic_info, on=['date', 'code'], how='outer')\n",
    "rawdata = ijoin(all_info, [return_info, basic_info])\n",
    "rawdata['date_int'] = rawdata['date'].copy()\n",
    "rawdata['date'] = pd.to_datetime(rawdata['date'], format='%Y%m%d')\n",
    "rawdata['daily_ret'] = rawdata['daily_ret'].astype(float)\n",
    "rawdata[feature_colns+['adjustment_daily_ret']] = rawdata[feature_colns+['adjustment_daily_ret']].astype(float)\n",
    "rawdata_copy = rawdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = rawdata_copy.copy()\n",
    "print('shape: ', rawdata.shape)\n",
    "print(rawdata[['code', 'date']].nunique())\n",
    "print(rawdata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdata = rawdata_copy.copy()\n",
    "# ts_codes = db.callTsFunc('getbk', u'800等权')\n",
    "# codes = [co[2:]+'.'+co[:2] for co in ts_codes]    \n",
    "# rawdata = rawdata[rawdata['code'].isin(codes)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 剔除样本\n",
    "rawdata = rawdata_copy.copy()\n",
    "if to_filter_code:  \n",
    "    idx_zt = rawdata['is_ZT'].astype(float) == 1\n",
    "    idx_dt = rawdata['is_DT'].astype(float) == 1\n",
    "    idx_nontd = rawdata['is_tradeday'].astype(float) == 0.0\n",
    "    idx_ST = rawdata['is_STstock'] == 1  # ST股\n",
    "    idx_new = rawdata['days_to_market'] <= 250  # 次新股\n",
    "    rawdata = rawdata[~(idx_zt | idx_dt | idx_nontd | idx_ST | idx_new)]\n",
    "#     rawdata = rawdata[~(idx_nontd | idx_ST | idx_new)]\n",
    "\n",
    "print('#rows: ', rawdata.shape)\n",
    "print(rawdata[['code', 'date']].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"构建建模数据\"\"\"\n",
    "from gabdnn import compile_3d_data, rebalance_by_class, batch_trimmer, ml_split\n",
    "rawdf = rawdata.copy()\n",
    "rawdata = gfc.compress_dataframe(rawdata[feature_colns + ['adjustment_daily_ret']].astype(float))\n",
    "idx_to_keep = np.array(rawdf['date'] > pd.to_datetime('20180301', format='%Y%m%d'))\n",
    "\n",
    "adj_ret = rawdf.pivot(index='date', columns='code', values='adjustment_daily_ret')\n",
    "shifted_ret = adj_ret.shift().stack().rename('shifted_ret')\n",
    "rawdf = rawdf.set_index(['date', 'code'])[feature_colns].join(shifted_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (456865, 21, 32)\n",
      "y:  (456865,)\n"
     ]
    }
   ],
   "source": [
    "rawdf_model = rawdf[~idx_to_keep].dropna()\n",
    "rawdf_keep = rawdf[idx_to_keep].dropna()\n",
    "X, y = rawdf_model[feature_colns].values, rawdf_model['shifted_ret'].values\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_snaps:  (456865, 21, 32)\n",
      "y_target:  (456865,)\n",
      "DescribeResult(nobs=456865, minmax=(-0.2891918208, 0.4408602151), mean=0.00038011577674893036, variance=0.00067575362094179, skewness=2.649637685661819, kurtosis=42.31603493868317)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window = 21\n",
    "X_snaps = gfc.rolling_window(X, window, axis=0)[:-1]\n",
    "y_target = y[window:]\n",
    "print('X_snaps: ', X_snaps.shape)\n",
    "print('y_target: ', y_target.shape)\n",
    "_ = plt.hist(y_target,bins=100)\n",
    "print(scs.describe(y_target))\n",
    "data_xs, data_ys = X_snaps, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    218843\n",
      "1    120365\n",
      "3    109862\n",
      "4      5577\n",
      "0      2218\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categorize and rebalance sample size\n",
    "# data_ys = np.digitize(data_ys, [9800, 10200])\n",
    "data_ys = np.digitize(data_ys, [-0.09, -0.01, 0.01, 0.09])\n",
    "print(pd.value_counts(data_ys))\n",
    "data_ys_copy = data_ys.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_sample = False\n",
    "if rebalance_sample:\n",
    "    new_sample = list(rebalance_by_class(data_ys, 'max'))\n",
    "    data_xs = np.vstack([data_xs[v] for _, v in new_sample])\n",
    "    data_ys = np.hstack([data_ys[v] for _, v in new_sample])\n",
    "    assert data_xs.shape[0] == data_ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_train  y_validate  y_test\n",
      "0   175159       21749   21933\n",
      "1   175217       21670   21953\n",
      "2   175038       22025   21777\n",
      "3   175006       21897   21935\n",
      "4   174940       22079   21822\n",
      "training set:  (875360, 21, 32)\n",
      "validate set:  (109420, 21, 32)\n",
      "test set:  (109420, 21, 32)\n",
      "total:  1094200\n"
     ]
    }
   ],
   "source": [
    "# 分组\n",
    "data_x, data_y = batch_trimmer(data_xs, data_ys, 100)\n",
    "training_x, validate_x, test_x = ml_split(data_x, [.8, .1, .1])\n",
    "training_y, validate_y, test_y = ml_split(data_y, [.8, .1, .1])\n",
    "\n",
    "# Categorize\n",
    "X_train, y_train = training_x.copy(), training_y.copy()\n",
    "X_validate, y_validate = validate_x.copy(), validate_y.copy()\n",
    "X_test, y_test = test_x.copy(), test_y.copy()\n",
    "print(pd.concat([pd.value_counts(eval(s)).rename(s) for s in ['y_train', 'y_validate', 'y_test']], 1))\n",
    "print('training set: ', training_x.shape)\n",
    "print('validate set: ', validate_x.shape)\n",
    "print('test set: ', test_x.shape)\n",
    "print('total: ', data_x.shape[0])\n",
    "assert training_x.shape[0] + validate_x.shape[0] + test_x.shape[0] == data_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(net, act=tf.nn.relu, shape=(2, 2, 1, 32), stride=(1, 1, 1, 1), \n",
    "               W_init=tf.truncated_normal_initializer(stddev=5e-2), b_init=tf.constant_initializer(value=0.0),\n",
    "               is_train=True, name='1'):\n",
    "    # The shape of the filters: (filter_height, filter_width, in_channels, out_channels).\n",
    "    name0 = 'conv/conv' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.Conv2dLayer(net, act=act, shape=shape, strides=stride, W_init=W_init, b_init=b_init, name=name0)\n",
    "    name1 = 'conv/bn' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name=name1)\n",
    "    return net\n",
    "\n",
    "\n",
    "def upsample_block(net, act=tf.nn.relu, out_shape=(5, 2, 32), stride=(1, 1, 1, 1), is_train=True, name='1'):\n",
    "    \"\"\"\n",
    "    Notes from tl.layers.DeConv2dLayer\n",
    "    - shape = [h, w, the number of output channels of this layer, the number of output channel of the previous layer].\n",
    "    - output_shape = [batch_size, any, any, the number of output channels of this layer].\n",
    "    - the number of output channel of a layer is its last dimension.\n",
    "\n",
    "    This function:\n",
    "    out_shape = (n_out_height, n_out_width, n_out_channel)\n",
    "    \"\"\"\n",
    "    n_out_height, n_out_width, n_out_channel = out_shape\n",
    "    batch_size, height, width, n_in_channel = net.inputs.shape\n",
    "    shape = (height.value, width.value, n_out_channel, n_in_channel.value)\n",
    "    output_shape = (batch_size.value, n_out_height, n_out_width, n_out_channel)\n",
    "\n",
    "    name0 = 'upsamp/deconv/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.DeConv2dLayer(net, act=act, shape=shape, output_shape=output_shape, strides=stride, name=name0)\n",
    "    name1 = 'upsamp/bn/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name=name1)\n",
    "    return net\n",
    "\n",
    "\n",
    "def downsample_block(net, filter_size=(3, 3), strides=(2, 2), padding='SAME', is_train=True, name='1'):\n",
    "    name0 = 'dnsamp/pool/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.MeanPool2d(net, filter_size=filter_size, strides=strides, padding=padding, name=name0)\n",
    "    name1 = 'dnsamp/bn/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name=name1)\n",
    "    return net\n",
    "\n",
    "\n",
    "def groupsample_block(net, n_filter=32, filter_size=(3, 3), strides=(2, 2), n_group=2, act=None, padding='SAME', is_train=True, name='1'):\n",
    "    name0 = 'gpsamp/pool/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.GroupConv2d(net, n_filter=32, filter_size=filter_size, strides=strides,\n",
    "                                n_group=n_group, act=act, padding=padding, name=name0)\n",
    "    name1 = 'gpsamp/bn/' + str('/' + name if len(name) else '')\n",
    "    net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name=name1)\n",
    "    return net\n",
    "\n",
    "\n",
    "def dense_block(net, n_units=100, act=None, is_train=True, name='1'):\n",
    "    name0 = 'dense_dense' + str('_' + name if len(name) else '')\n",
    "    try:\n",
    "        net = tl.layers.DenseLayer(net, n_units=n_units, act=act, name=name0)\n",
    "    except AssertionError as e:\n",
    "        print('[Warning]', name0, ': ', e)\n",
    "        net = tl.layers.FlattenLayer(net, name='dense/flatten' + str('_' + name if len(name) else ''))\n",
    "        net = tl.layers.DenseLayer(net, n_units=n_units, act=act, name=name0)\n",
    "    name1 = 'dense_bn' + str('_' + name if len(name) else '')\n",
    "    net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name=name1)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "def model(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"binarynet\", reuse=reuse):\n",
    "\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        net = tl.layers.InputLayer(x, name='input')\n",
    "\n",
    "        # Block\n",
    "        # net = upsample_block(net, out_shape=(3, 5, 3), act=None, is_train=is_train, name='1')\n",
    "        net = conv_block(net, shape=(2, 2, 3, 32), act=None, is_train=is_train, \n",
    "                         W_init=xavier_init, b_init=None, name='1')\n",
    "        net = downsample_block(net, filter_size=(2, 2), strides=(1, 1), padding='VALID', is_train=is_train, name='1')\n",
    "        net = conv_block(net, shape=(2, 2, 32, 32), act=None, is_train=is_train, name='2')\n",
    "\n",
    "        # FC Layer\n",
    "        net = dense_block(net, n_units=100, act=None, is_train=is_train, name='1')\n",
    "        net = dense_block(net, n_units=5, act=tf.nn.softmax, is_train=is_train, name='2')\n",
    "        \n",
    "    return net\n",
    "\n",
    "\n",
    "def simple_model1(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope('simple_model1', reuse=reuse):\n",
    "        pass\n",
    "\n",
    "\n",
    "def simple_model2(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope('simple_model2', reuse=reuse):\n",
    "        pass\n",
    "\n",
    "\n",
    "def ResNet(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope('ResNet', reuse=reuse):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def accuracy(X, y, batch_size, n=None, shuffle=True):\n",
    "    loss, acc = 0, 0\n",
    "    for n_batch, (Xs, ys) in enumerate(tl.iterate.minibatches(X, y, batch_size=batch_size, shuffle=shuffle)):\n",
    "        err, ac = sess.run([tf_pred_loss, tf_accuracy], feed_dict={x: Xs, y_: ys})\n",
    "        loss += err\n",
    "        acc += ac\n",
    "    return loss / n_batch, acc / n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = None\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "    tf.InteractiveSession.close(sess)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "batch_size = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  binarynet/input: (21, 21, 1, 32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-0044fe7a6651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# b1 = tf.Variable(initializer([3, 5]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnet_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-6383e417bc1b>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(x, is_train, reuse)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# net = upsample_block(net, out_shape=(3, 5, 3), act=None, is_train=is_train, name='1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         net = conv_block(net, shape=(2, 2, 3, 32), act=None, is_train=is_train, \n\u001b[0;32m---> 74\u001b[0;31m                          W_init=xavier_init, b_init=None, name='1')\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-6383e417bc1b>\u001b[0m in \u001b[0;36mconv_block\u001b[0;34m(net, act, shape, stride, W_init, b_init, is_train, name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# The shape of the filters: (filter_height, filter_width, in_channels, out_channels).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'conv/conv'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2dLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mname1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'conv/bn'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mrename_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maliases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_support_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/layers/convolution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prev_layer, act, shape, strides, padding, W_init, b_init, W_init_args, b_init_args, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    204\u001b[0m         logging.info(\n\u001b[1;32m    205\u001b[0m             \u001b[0;34m\"Conv2dLayer %s: shape:%s strides:%s pad:%s act:%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         )\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # define inferences\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, 21, 1, 32])\n",
    "    y_ = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "\n",
    "    # initializer = tf.contrib.layers.xavier_initializer()\n",
    "    # w1 = tf.Variable(initializer([3, 5]))\n",
    "    # b1 = tf.Variable(initializer([3, 5]))\n",
    "\n",
    "    net_train = model(x, is_train=True, reuse=False)\n",
    "    net_test = model(x, is_train=False, reuse=True)\n",
    "\n",
    "    # cost for training\n",
    "    train_y = net_train.outputs\n",
    "    tf_train_loss = tl.cost.cross_entropy(train_y, y_, name='xentropy')\n",
    "\n",
    "    # cost and accuracy for evalution\n",
    "    test_y = net_test.outputs\n",
    "    tf_pred_loss = tl.cost.cross_entropy(test_y, y_, name='xentropy2')\n",
    "    tf_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_y, 1), y_), tf.float32))\n",
    "\n",
    "    # define the optimizer\n",
    "    train_params = tl.layers.get_variables_with_name('binarynet', True, True)\n",
    "    optimizer = tl.optimizers.AMSGrad(learning_rate=0.005)\n",
    "    # grads_and_vars = optimizer.compute_gradients(tf_train_loss)\n",
    "    tf_loss_minimize = optimizer.minimize(tf_train_loss, var_list=train_params)\n",
    "    # tf_loss_minimize = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "n_epoch = 50\n",
    "performances_per_epoch = []\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print('\\nStrating epoch {}'.format(epoch))\n",
    "    start_time = datetime.now()\n",
    "    batches_i = 0\n",
    "    loss_per_epoch = []\n",
    "    for xs, ys in tl.iterate.minibatches(X_train, y_train, batch_size=batch_size, shuffle=True):\n",
    "        # training for batches\n",
    "        if batches_i == 0:\n",
    "            _, loss, acc = sess.run(\n",
    "                [tf_loss_minimize, tf_train_loss, tf_accuracy],\n",
    "                feed_dict={x: xs, y_: ys}\n",
    "            )\n",
    "            # summ_writer.add_summary(gn_summ, epoch)\n",
    "            # summ_writer.add_summary(wb_summ, epoch)\n",
    "        else:\n",
    "            _, loss, acc = sess.run(\n",
    "                [tf_loss_minimize, tf_train_loss, tf_accuracy],\n",
    "                feed_dict={x: xs, y_: ys}\n",
    "            )\n",
    "        loss_per_epoch.append(loss)\n",
    "        batches_i += 1\n",
    "\n",
    "    try:\n",
    "        print('Average loss in epoch %d: %.5f' % (epoch, np.mean(loss_per_epoch)))\n",
    "        avg_loss = np.mean(loss_per_epoch)\n",
    "    except Exception as e:\n",
    "        print('Epoch {}: '.format(epoch), e)\n",
    "        pass\n",
    "\n",
    "    print(\"Epoch %d of %d took %.4fs\" % (epoch, n_epoch, (datetime.now() - start_time).total_seconds()))\n",
    "\n",
    "    print('Training Set')\n",
    "    train_loss, train_acc = accuracy(X_train, y_train, batch_size, shuffle=True)\n",
    "    print(\"   train loss: \", train_loss)\n",
    "    print(\"   train acc : \", train_acc)\n",
    "\n",
    "    print('Validation Set')\n",
    "    validate_loss, validate_acc = accuracy(X_validate, y_validate, batch_size, shuffle=True)\n",
    "    print(\"   validation loss: \", validate_loss)\n",
    "    print(\"   validation acc : \", validate_acc)\n",
    "\n",
    "    print('Test Set')\n",
    "    test_loss, test_acc = accuracy(X_test, y_test, batch_size, shuffle=True)\n",
    "    print(\"   test loss: \", test_loss)\n",
    "    print(\"   test acc : \", test_acc)\n",
    "\n",
    "    # Execute the summaries defined above\n",
    "    # performances_per_epoch.append([train_loss, train_acc, validate_loss, validate_acc, test_loss, test_acc])\n",
    "    # summ = sess.run(performance_summaries, feed_dict={tf_loss_ph: train_loss, tf_accuracy_ph: validate_loss})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the TensorBoard\n",
    "    # summ_writer.add_summary(summ, epoch)\n",
    "\n",
    "# save_path = tf.train.Saver().save(sess=sess, save_path=model_dir + '/model.ckpt')\n",
    "# print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
