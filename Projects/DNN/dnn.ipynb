{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __context__ import *\n",
    "from __future__ import print_function, unicode_literals, division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_delta(df, lookforward=5):\n",
    "    op, hi, lo, cl, pre_cl = df[['open', 'high', 'low', 'new_price', 'pre_close']].values.T\n",
    "    d_arr = np.stack([op, hi, lo, cl], 1)\n",
    "    d_arr = ((d_arr/pre_cl.reshape(-1, 1) - 1) * 1000).astype(int) + 100\n",
    "    d_oc = (d_arr[:, 3] - d_arr[:, 0]).reshape(-1, 1)\n",
    "    expr = np.full(d_oc.shape, np.nan)\n",
    "    expr[:-lookforward, 0] = cl[lookforward:] / cl[:-lookforward] - 1\n",
    "    out_df = pd.DataFrame(np.hstack([d_arr, d_oc]), index=df.index, \n",
    "                          columns=['d_open', 'd_high', 'd_low', 'd_close', 'd_oc'])\n",
    "    out_df.loc[:, 'expr03'] = expr\n",
    "    return out_df.dropna()\n",
    "\n",
    "def compile_data(data_d):\n",
    "    channel_1 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    channel_2 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_begin = np.arange(data_d.shape[0]) * 201\n",
    "\n",
    "    lb1 = data_d[['d_open', 'd_close']].min(1).values\n",
    "    ub1 = data_d[['d_open', 'd_close']].max(1).values + 1\n",
    "    channel_1 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_to_fill_chan1 = [np.arange(begi, endi) for begi, endi in zip(idx_begin + lb1, idx_begin + ub1)]\n",
    "    np.put(channel_1, np.hstack(idx_to_fill_chan1), 1)\n",
    "\n",
    "    lb2 = data_d['d_low'].values\n",
    "    ub2 = data_d['d_high'].values + 1\n",
    "    channel_2 = np.zeros(shape=(data_d.shape[0], 201))\n",
    "    idx_to_fill_chan2 = [np.arange(begi, endi) for begi, endi in zip(idx_begin + lb2, idx_begin + ub2)]\n",
    "    np.put(channel_2, np.hstack(idx_to_fill_chan2), 1)\n",
    "\n",
    "    features = np.stack((channel_1, channel_2), axis=-1)\n",
    "    # print('feature shape', features.shape)\n",
    "    return features\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def get_data_batch_2d(features, ys, time_step):\n",
    "    features_T = features.transpose((1, 2, 0))\n",
    "    features_T_rol = rolling_window(features_T, time_step)\n",
    "    data_x = features_T_rol.transpose(2, 3, 0, 1)\n",
    "    data_y = ys[time_step-1:]\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "def get_training_data(quota, batch_size=100, period_step=20, predict_period=5):\n",
    "    data2 = quota.copy()\n",
    "    predict_data2 = get_data_delta(data2, lookforward=predict_period)\n",
    "\n",
    "    predict_features2 = compile_data(predict_data2)\n",
    "    _ys2 = predict_data2.loc[:, 'expr03'].values\n",
    "    predict_x2, predict_dump2 = get_data_batch_2d(predict_features2, _ys2, period_step)\n",
    "    rest = batch_size - (predict_x2.shape[0] % batch_size)\n",
    "    # print('predict_x:', predict_x2.shape)\n",
    "\n",
    "    predict_x2 = np.concatenate([predict_x2, predict_x2[:rest]])\n",
    "    predict_dump2 = np.concatenate([predict_dump2, predict_dump2[:rest]])\n",
    "    return predict_x2, predict_dump2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取行情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "begdate = 20080601\n",
    "enddate = 20180601\n",
    "code = '000002.SZ'\n",
    "predict_period=5\n",
    "\n",
    "DB = gcrl.db('internal')\n",
    "DB.query_init({'code': code, 'begin': begdate, 'end': enddate, 'fields': '*'})\n",
    "DB.query.update({'dtype':'cycle', 'n_cycle': 1, 'freq': pycoraldb.D})\n",
    "rawdf = DB.getBar(4)\n",
    "rawdf['close'] = rawdf['new_price'].copy()\n",
    "# rawdf = db.getBar(code, 20180501, 20180601).toDataFrame()\n",
    "# gclean = gfc.clean_rdata(rawdf)\n",
    "# clean_df = gclean.remove_by_time('stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data(rawdf, window=3, lookforward=1, verify=True):\n",
    "    # get X\n",
    "    df = rawdf.loc[rawdf['status']==0, ['open', 'high', 'low', 'new_price', 'new_volume']].dropna()\n",
    "    delta = df.pct_change().dropna().values\n",
    "    # delta = delta[~np.isinf(delta).any(1)] * 10000 + 10000   \n",
    "    X = (10000 * (delta + 1)).astype(int)\n",
    "    # get y\n",
    "    rtn = rawdf.loc[rawdf['status']==0, 'new_price'].pct_change(lookforward).dropna().values\n",
    "    y = (10000 * (rtn + 1)).astype(int)\n",
    "    # Generat snapshots with targets\n",
    "    X_snaps = rolling_window(X.T, window).transpose(1, 2, 0)\n",
    "    X_snaps = X_snaps.reshape(X_snaps.shape+(1,))[1:]\n",
    "    y_target = y[window:]\n",
    "    if verify:\n",
    "        print('Before rolling: ', X.shape, y.shape)\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        print('After rolling: ', X_snaps.shape, y_target.shape)\n",
    "        assert X_snaps.shape[0] == y_target.shape[0]\n",
    "    return X_snaps, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_trimmer(X, y, batch_size=100):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    trim_n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:trim_n], y[:trim_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = compile_data(rawdf, window=3, lookforward=1, verify=False)\n",
    "data_x, data_y = batch_trimmer(data_x, data_y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集x (1760, 3, 5, 1)\n",
      "测试集x (440, 3, 5, 1)\n",
      "总共x 2200\n"
     ]
    }
   ],
   "source": [
    "training_count = int(data_x.shape[0] * 0.8)\n",
    "test_count = data_x.shape[0] - training_count\n",
    "\n",
    "training_x = data_x[:training_count]\n",
    "training_y = data_y[:training_count]\n",
    "test_x = data_x[training_count:]\n",
    "test_y = data_y[training_count:]\n",
    "\n",
    "print('训练集x', training_x.shape)\n",
    "print('测试集x', test_x.shape)\n",
    "print('总共x', data_x.shape[0])\n",
    "assert(training_x.shape[0] + test_x.shape[0] == data_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = training_x.copy(), training_y.copy(), test_x.copy(), test_y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train > 10500, 1, np.where(y_train < 9700, 2, 0)).astype(int)\n",
    "y_test = np.where(y_test > 10500, 1, np.where(y_test < 9700, 2, 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, is_train=True, reuse=False):\n",
    "    # In BNN, all the layers inputs are binary, with the exception of the first layer.\n",
    "    # ref: https://github.com/itayhubara/BinaryNet.tf/blob/master/models/BNN_cifar10.py\n",
    "    with tf.variable_scope(\"binarynet\", reuse=reuse):\n",
    "        \n",
    "        net = tl.layers.InputLayer(x, name='input')\n",
    " \n",
    "        net = tl.layers.Conv2d(net, 32, (2, 2), (1, 1), act=tf.nn.relu, padding='SAME', name='cnn2d')\n",
    "        # net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool0')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn0')\n",
    "        \n",
    "        net = tl.layers.BinaryConv2d(net, 32, (2, 2), (1, 1),\n",
    "                                     padding='SAME', b_init=None, name='bcnn1')\n",
    "        net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool1')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn1')\n",
    "\n",
    "        net = tl.layers.BinaryConv2d(net, 64, (2, 2), (1, 1),\n",
    "                                     padding='SAME', b_init=None, name='bcnn2')\n",
    "        net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding='SAME', name='pool2')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn2')\n",
    "\n",
    "        net = tl.layers.FlattenLayer(net)\n",
    "        # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop1')\n",
    "        # net = tl.layers.SignLayer(net)\n",
    "        net = tl.layers.BinaryDenseLayer(net, 5, b_init=None, name='dense')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tl.act.htanh, is_train=is_train, name='bn3')\n",
    "\n",
    "        # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name='drop2')\n",
    "        # net = tl.layers.SignLayer(net)\n",
    "        net = tl.layers.BinaryDenseLayer(net, 3, b_init=None, name='bout')\n",
    "        net = tl.layers.BatchNormLayer(net, act=tf.nn.sigmoid, is_train=is_train, name='bno')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = None\n",
    "if sess == None:\n",
    "    sess = tf.InteractiveSession()\n",
    "elif sess._closed == False:\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.InteractiveSession.close(sess)\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  binarynet/input: (100, 3, 5, 1)\n",
      "[TL] Conv2d binarynet/cnn2d: n_filter:32 filter_size:(2, 2) strides:(1, 1) pad:SAME act:relu\n",
      "[TL] BatchNormLayer bn0: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] BinaryConv2d bcnn1: n_filter:32 filter_size:(2, 2) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool1: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn1: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] BinaryConv2d bcnn2: n_filter:64 filter_size:(2, 2) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool2: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn2: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] FlattenLayer binarynet/flatten: 128\n",
      "[TL] BinaryDenseLayer  dense: 5 identity\n",
      "[TL] BatchNormLayer bn3: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:True\n",
      "[TL] BinaryDenseLayer  bout: 3 identity\n",
      "[TL] BatchNormLayer bno: decay:0.900000 epsilon:0.000010 act:sigmoid is_train:True\n",
      "[TL] InputLayer  binarynet/input: (100, 3, 5, 1)\n",
      "[TL] Conv2d binarynet/cnn2d: n_filter:32 filter_size:(2, 2) strides:(1, 1) pad:SAME act:relu\n",
      "[TL] BatchNormLayer bn0: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] BinaryConv2d bcnn1: n_filter:32 filter_size:(2, 2) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool1: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn1: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] BinaryConv2d bcnn2: n_filter:64 filter_size:(2, 2) strides:(1, 1) pad:SAME act:identity\n",
      "[TL] MaxPool2d pool2: filter_size:(2, 2) strides:(2, 2) padding:SAME\n",
      "[TL] BatchNormLayer bn2: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] FlattenLayer binarynet/flatten: 128\n",
      "[TL] BinaryDenseLayer  dense: 5 identity\n",
      "[TL] BatchNormLayer bn3: decay:0.900000 epsilon:0.000010 act:hard_tanh is_train:False\n",
      "[TL] BinaryDenseLayer  bout: 3 identity\n",
      "[TL] BatchNormLayer bno: decay:0.900000 epsilon:0.000010 act:sigmoid is_train:False\n"
     ]
    }
   ],
   "source": [
    "# define inferences\n",
    "batch_size = 100\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, 3, 5, 1])\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "net_train = model(x, is_train=True, reuse=False)\n",
    "net_test = model(x, is_train=False, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   [*] geting variables with binarynet\n",
      "[TL]   got   0: binarynet/cnn2d/kernel:0   (2, 2, 1, 32)\n",
      "[TL]   got   1: binarynet/cnn2d/bias:0   (32,)\n",
      "[TL]   got   2: binarynet/bn0/beta:0   (32,)\n",
      "[TL]   got   3: binarynet/bn0/gamma:0   (32,)\n",
      "[TL]   got   4: binarynet/bcnn1/W_conv2d:0   (2, 2, 32, 32)\n",
      "[TL]   got   5: binarynet/bn1/beta:0   (32,)\n",
      "[TL]   got   6: binarynet/bn1/gamma:0   (32,)\n",
      "[TL]   got   7: binarynet/bcnn2/W_conv2d:0   (2, 2, 32, 64)\n",
      "[TL]   got   8: binarynet/bn2/beta:0   (64,)\n",
      "[TL]   got   9: binarynet/bn2/gamma:0   (64,)\n",
      "[TL]   got  10: binarynet/dense/W:0   (128, 5)\n",
      "[TL]   got  11: binarynet/bn3/beta:0   (5,)\n",
      "[TL]   got  12: binarynet/bn3/gamma:0   (5,)\n",
      "[TL]   got  13: binarynet/bout/W:0   (5, 3)\n",
      "[TL]   got  14: binarynet/bno/beta:0   (3,)\n",
      "[TL]   got  15: binarynet/bno/gamma:0   (3,)\n"
     ]
    }
   ],
   "source": [
    "# cost for training\n",
    "y = net_train.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "\n",
    "# cost and accuracy for evalution\n",
    "y2 = net_test.outputs\n",
    "prediction = tf.argmax(y2, 1)\n",
    "\n",
    "cost_test = tl.cost.cross_entropy(y2, y_, name='xentropy2')\n",
    "correct_prediction = tf.equal(tf.argmax(y2, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# define the optimizer\n",
    "train_params = tl.layers.get_variables_with_name('binarynet', True, True)\n",
    "# train_op = tl.optimizers.AMSGrad(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n",
    "\n",
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   param   0: binarynet/cnn2d/kernel:0 (2, 2, 1, 32)      float32_ref (mean: 0.0016860561445355415, median: 0.001863574841991067, std: 0.01633264683187008)   \n",
      "[TL]   param   1: binarynet/cnn2d/bias:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: binarynet/bn0/beta:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   3: binarynet/bn0/gamma:0 (32,)              float32_ref (mean: 0.9993168711662292, median: 0.9993283152580261, std: 0.0022547768894582987)   \n",
      "[TL]   param   4: binarynet/bn0/moving_mean:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   5: binarynet/bn0/moving_variance:0 (32,)              float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param   6: binarynet/bcnn1/Sign:0 (2, 2, 32, 32)     float32 (mean: 0.0068359375      , median: 1.0               , std: 0.999976634979248 )   \n",
      "[TL]   param   7: binarynet/bn1/beta:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   8: binarynet/bn1/gamma:0 (32,)              float32_ref (mean: 0.999971866607666 , median: 0.9997364282608032, std: 0.002783637959510088)   \n",
      "[TL]   param   9: binarynet/bn1/moving_mean:0 (32,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  10: binarynet/bn1/moving_variance:0 (32,)              float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param  11: binarynet/bcnn2/Sign:0 (2, 2, 32, 64)     float32 (mean: -0.004638671875   , median: -1.0              , std: 0.9999892115592957)   \n",
      "[TL]   param  12: binarynet/bn2/beta:0 (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  13: binarynet/bn2/gamma:0 (64,)              float32_ref (mean: 1.000083327293396 , median: 0.9996504187583923, std: 0.0022600784432142973)   \n",
      "[TL]   param  14: binarynet/bn2/moving_mean:0 (64,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  15: binarynet/bn2/moving_variance:0 (64,)              float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param  16: binarynet/dense/Sign:0 (128, 5)           float32 (mean: -0.04374999925494194, median: -1.0              , std: 0.9990425109863281)   \n",
      "[TL]   param  17: binarynet/bn3/beta:0 (5,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  18: binarynet/bn3/gamma:0 (5,)               float32_ref (mean: 1.0005531311035156, median: 1.0006378889083862, std: 0.0019644086714833975)   \n",
      "[TL]   param  19: binarynet/bn3/moving_mean:0 (5,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  20: binarynet/bn3/moving_variance:0 (5,)               float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   param  21: binarynet/bout/Sign:0 (5, 3)             float32 (mean: -0.7333333492279053, median: -1.0              , std: 0.6798692941665649)   \n",
      "[TL]   param  22: binarynet/bno/beta:0 (3,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  23: binarynet/bno/gamma:0 (3,)               float32_ref (mean: 1.0009676218032837, median: 1.0009485483169556, std: 0.0008421692182309926)   \n",
      "[TL]   param  24: binarynet/bno/moving_mean:0 (3,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param  25: binarynet/bno/moving_variance:0 (3,)               float32_ref (mean: 1.0               , median: 1.0               , std: 0.0               )   \n",
      "[TL]   num of params: 13647\n"
     ]
    }
   ],
   "source": [
    "# net_train.print_params()\n",
    "# net_train.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 1.443077s\n",
      "   train loss: 1.090999\n",
      "   train acc: 0.408824\n",
      "Evaluation\n",
      "   test loss: 1.091313\n",
      "   test acc: 0.410000\n",
      "Epoch 2 of 10 took 1.317194s\n",
      "   train loss: 1.093207\n",
      "   train acc: 0.277059\n",
      "Evaluation\n",
      "   test loss: 1.094958\n",
      "   test acc: 0.282500\n",
      "Epoch 3 of 10 took 1.397819s\n",
      "   train loss: 1.045633\n",
      "   train acc: 0.546471\n",
      "Evaluation\n",
      "   test loss: 1.051365\n",
      "   test acc: 0.522500\n",
      "Epoch 4 of 10 took 1.264396s\n",
      "   train loss: 1.042805\n",
      "   train acc: 0.614706\n",
      "Evaluation\n",
      "   test loss: 1.038184\n",
      "   test acc: 0.620000\n",
      "Epoch 5 of 10 took 1.387047s\n",
      "   train loss: 1.086467\n",
      "   train acc: 0.388824\n",
      "Evaluation\n",
      "   test loss: 1.084534\n",
      "   test acc: 0.402500\n",
      "Epoch 6 of 10 took 1.158743s\n",
      "   train loss: 1.046477\n",
      "   train acc: 0.543529\n",
      "Evaluation\n",
      "   test loss: 1.046219\n",
      "   test acc: 0.562500\n",
      "Epoch 7 of 10 took 1.359743s\n",
      "   train loss: 1.060904\n",
      "   train acc: 0.542941\n",
      "Evaluation\n",
      "   test loss: 1.054475\n",
      "   test acc: 0.560000\n",
      "Epoch 8 of 10 took 1.211658s\n",
      "   train loss: 1.041197\n",
      "   train acc: 0.659412\n",
      "Evaluation\n",
      "   test loss: 1.040212\n",
      "   test acc: 0.652500\n",
      "Epoch 9 of 10 took 1.395810s\n",
      "   train loss: 1.082079\n",
      "   train acc: 0.529412\n",
      "Evaluation\n",
      "   test loss: 1.076300\n",
      "   test acc: 0.555000\n",
      "Epoch 10 of 10 took 1.424890s\n",
      "   train loss: 1.070002\n",
      "   train acc: 0.588824\n",
      "Evaluation\n",
      "   test loss: 1.064529\n",
      "   test acc: 0.607500\n",
      "Evaluation\n",
      "   test loss: 1.071003\n",
      "   test acc: 0.600000\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "seq_l = 5\n",
    "bas = batch_size // seq_l\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    for X_train_a, y_train_a in tl.iterate.seq_minibatches(X_train, y_train, batch_size=bas, seq_length=seq_l):\n",
    "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
    "\n",
    "    if True:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "        print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "        \n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "#         for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "#             err, ac = sess.run([cost_test, acc], feed_dict={x: X_val_a, y_: y_val_a})\n",
    "#             val_loss += err\n",
    "#             val_acc += ac\n",
    "#             n_batch += 1\n",
    "#         print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "#         print(\"   val acc: %f\" % (val_acc / n_batch))\n",
    "\n",
    "        print('Evaluation')\n",
    "        test_loss, test_acc, n_batch = 0, 0, 0\n",
    "        for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "            test_loss += err\n",
    "            test_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "        print(\"   test acc: %f\" % (test_acc / n_batch))\n",
    "#         clear_output()\n",
    "\n",
    "\n",
    "print('Evaluation')\n",
    "test_loss, test_acc, n_batch = 0, 0, 0\n",
    "for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "    err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "    test_loss += err\n",
    "    test_acc += ac\n",
    "    n_batch += 1\n",
    "\n",
    "print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "print(\"   test acc: %f\" % (test_acc / n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测与实际图对比\n",
    "#fig, axs = plt.subplots(1,1,figsize = (25,8))\n",
    "\n",
    "# predict_x, predict_dump = compile_data(rawdf, 3, 1, True)\n",
    "# predict_x, predict_dump = batch_trimmer(predict_x, predict_dump)\n",
    "\n",
    "real_x, real_y = batch_trimmer(X_test, y_test)\n",
    "pred_y = []\n",
    "for X_train_a, y_train_a in tl.iterate.minibatches(real_x, real_y, 100, shuffle=False):\n",
    "#     print('batch predict')\n",
    "    # pred.append(sess.run(prediction, feed_dict={x: X_train_a}))\n",
    "    pred_y.append(sess.run(y2, feed_dict={x: X_train_a}))\n",
    "\n",
    "pred_y = np.vstack(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_y: (400,)\n",
      "pred_x: (400, 3)\n"
     ]
    }
   ],
   "source": [
    "print('real_y:', real_y.shape)\n",
    "# print('predict_x:', predict_x.shape)\n",
    "print('pred_y:', pred_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_x[:, 0] - real_y\n",
    "sum(pred_y.argmax(1) == real_y) / real_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "uuid": "0bc32a8f-be64-43a0-85fb-675271842356"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
